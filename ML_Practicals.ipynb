{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "222e07cf-0eca-4fef-9d44-24fc211c9d14",
   "metadata": {},
   "source": [
    "<h1>Practical 1: Implement Linear Regression (Diabetes Dataset)</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d64687-3d83-420e-a5f3-5ef36993cc9c",
   "metadata": {},
   "source": [
    "pip install numpy<br>\n",
    "pip install scikit-learn<br>\n",
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7436619-cf56-4874-bcbf-8a7f0465d146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coefficents:\n",
      " [924.88539574]\n",
      "mean squared error: 3577.73\n",
      " coefficent of determination (R² score) : 0.368161\n"
     ]
    },
    {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "diabetes = datasets.load_diabetes()\n",
    "diabetes_X = diabetes.data[:, np.newaxis, 2]\n",
    "diabetes_X_train, diabetes_X_test, diabetes_y_train, diabetes_y_test = train_test_split(\n",
    "    diabetes_X, diabetes.target, test_size=0.3\n",
    ")\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(diabetes_X_train,diabetes_y_train)\n",
    "diabetes_y_pred = regr.predict(diabetes_X_test)\n",
    "print('coefficents:\\n',regr.coef_)\n",
    "print('mean squared error: %.2f' % mean_squared_error(diabetes_y_test, diabetes_y_pred))\n",
    "print (' coefficent of determination (R² score) : %2f' % r2_score(diabetes_y_test, diabetes_y_pred))\n",
    "plt.scatter(diabetes_X_test,diabetes_y_test,color='black')\n",
    "plt.plot(diabetes_X_test,diabetes_y_pred,color='blue',linewidth=3)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.xlabel('BMI')\n",
    "plt.ylabel('disease progression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e5481d-3ba0-4def-93b3-a31d87c38b63",
   "metadata": {},
   "source": [
    "<h1>Practical 2: Implement Logistic Regression (Iris Dataset)</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dc82a3-2a04-4b77-977e-f92a3ca72b75",
   "metadata": {},
   "source": [
    "pip install numpy<br>\n",
    "pip install scikit-learn<br>\n",
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "445a3f50-f915-4f36-b68b-b00cb1888ee5",
   "metadata": {},
   "outputs": [
    {
     "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import datasets\n",
    "\n",
    "iris =datasets.load_iris()\n",
    "\n",
    "X=iris.data[:, :2]\n",
    "Y=iris.target\n",
    "\n",
    "logreg= LogisticRegression(C=1e5)\n",
    "logreg.fit(X,Y)\n",
    "x_min,x_max = X[:,0].min() -0.5, X[:,0].max() + 0.5\n",
    "y_min,y_max = X[:,1].min() -0.5, X[:,0].max() + 0.5\n",
    "\n",
    "h = 0.02\n",
    "xx,yy = np.meshgrid(np.arange(x_min,x_max,h),\n",
    "                    np.arange(y_min,y_max))\n",
    "\n",
    "Z= logreg.predict(np.c_[xx.ravel(),yy.ravel()])\n",
    "Z=Z.reshape(xx.shape)\n",
    "plt.figure(1,figsize=(4,3))\n",
    "plt.pcolormesh(xx,yy,Z,cmap=plt.cm.Paired)\n",
    "plt.scatter(X[:,0],X[:,1],c=Y,edgecolors='k',cmap=plt.cm.Paired)\n",
    "plt.xlabel(\"sepal length\")\n",
    "plt.ylabel(\"sepal width\")\n",
    "plt.title(\"Logistic Regression on Iris dataset\")\n",
    "plt.xlim(xx.min(),xx.max())\n",
    "plt.ylim(yy.min(),yy.max())\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1c5f56-8136-40d8-b3b6-d16480d40d06",
   "metadata": {},
   "source": [
    "<h1>Practical 3: Implement Multinomial Logistic Regression (Iris Dataset) </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e009dcc3-129b-4c8d-ab28-c14747cf8182",
   "metadata": {},
   "source": [
    "pip install numpy<br>\n",
    "pip install scikit-learn<br>\n",
    "pip install matplotlib<br>\n",
    "pip install pandas<br>\n",
    "pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30c0886d-02a2-475b-ad28-7fa375fef689",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00\n",
      "Error rate: 0.00\n",
      "Cross-Validation Accuracy: 0.97 (+/- 0.10)\n",
      "\n",
      "Confusion Matrix:\n",
      " [[10  0  0]\n",
      " [ 0  9  0]\n",
      " [ 0  0 11]]\n"
     ]
    },
    {
     "text/plain": [
       "<Figure size 480x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted Probabilities:\n",
      " [[3.79689810e-03 8.27256284e-01 1.68946818e-01]\n",
      " [9.46826471e-01 5.31733295e-02 1.99615578e-07]\n",
      " [8.77983039e-09 1.55376783e-03 9.98446223e-01]\n",
      " [6.45430989e-03 7.92156630e-01 2.01389060e-01]\n",
      " [1.44797015e-03 7.74218420e-01 2.24333610e-01]\n",
      " [9.55789632e-01 4.42101918e-02 1.76612438e-07]\n",
      " [7.77234132e-02 9.07985388e-01 1.42911991e-02]\n",
      " [1.62249176e-04 1.55484429e-01 8.44353322e-01]\n",
      " [2.21306755e-03 7.62981425e-01 2.34805508e-01]\n",
      " [2.83643420e-02 9.45774028e-01 2.58616301e-02]\n",
      " [4.41748370e-04 2.43099926e-01 7.56458326e-01]\n",
      " [9.68245846e-01 3.17540756e-02 7.80357098e-08]\n",
      " [9.72953124e-01 2.70468424e-02 3.32903521e-08]\n",
      " [9.62040200e-01 3.79596893e-02 1.10857725e-07]\n",
      " [9.79278911e-01 2.07210242e-02 6.46695857e-08]\n",
      " [4.56298685e-03 7.12455809e-01 2.82981204e-01]\n",
      " [7.26039927e-06 2.41851486e-02 9.75807591e-01]\n",
      " [2.73519473e-02 9.47707486e-01 2.49405666e-02]\n",
      " [8.25015848e-03 8.31230547e-01 1.60519295e-01]\n",
      " [1.42579408e-05 3.59274345e-02 9.64058308e-01]\n",
      " [9.64308215e-01 3.56915925e-02 1.92780812e-07]\n",
      " [1.31882506e-03 3.99077374e-01 5.99603801e-01]\n",
      " [9.61608567e-01 3.83911723e-02 2.60923269e-07]\n",
      " [1.86247863e-05 4.58478161e-02 9.54133559e-01]\n",
      " [1.64911556e-06 2.57697037e-02 9.74228647e-01]\n",
      " [9.37130111e-05 1.04959315e-01 8.94946972e-01]\n",
      " [8.72941282e-06 5.83344427e-02 9.41656828e-01]\n",
      " [4.32172080e-06 1.88238400e-02 9.81171838e-01]\n",
      " [9.66777950e-01 3.32219145e-02 1.35734961e-07]\n",
      " [9.56232932e-01 4.37668354e-02 2.32294542e-07]]\n",
      "Manual Calculated Accuracy is: 100.00%\n",
      "\n",
      "Misclassified Predictions:\n",
      " Empty DataFrame\n",
      "Columns: [0, 1, 2, sum, predicted_class, actual_class, correct_prediction?]\n",
      "Index: []\n",
      "Optimization terminated successfully.\n",
      "         Current function value: nan\n",
      "         Iterations 26\n",
      "                          MNLogit Regression Results                          \n",
      "==============================================================================\n",
      "Dep. Variable:                species   No. Observations:                  150\n",
      "Model:                        MNLogit   Df Residuals:                      140\n",
      "Method:                           MLE   Df Model:                            8\n",
      "Date:                Wed, 09 Jul 2025   Pseudo R-squ.:                     nan\n",
      "Time:                        13:05:16   Log-Likelihood:                    nan\n",
      "converged:                       True   LL-Null:                       -164.79\n",
      "Covariance Type:            nonrobust   LLR p-value:                       nan\n",
      "=====================================================================================\n",
      "        species=1       coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------\n",
      "const                    nan        nan        nan        nan         nan         nan\n",
      "sepal length (cm)        nan        nan        nan        nan         nan         nan\n",
      "sepal width (cm)         nan        nan        nan        nan         nan         nan\n",
      "petal length (cm)        nan        nan        nan        nan         nan         nan\n",
      "petal width (cm)         nan        nan        nan        nan         nan         nan\n",
      "-------------------------------------------------------------------------------------\n",
      "        species=2       coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------\n",
      "const                    nan        nan        nan        nan         nan         nan\n",
      "sepal length (cm)        nan        nan        nan        nan         nan         nan\n",
      "sepal width (cm)         nan        nan        nan        nan         nan         nan\n",
      "petal length (cm)        nan        nan        nan        nan         nan         nan\n",
      "petal width (cm)         nan        nan        nan        nan         nan         nan\n",
      "=====================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:3027: RuntimeWarning: overflow encountered in exp\n",
      "  eXB = np.column_stack((np.ones(len(X)), np.exp(X)))\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:3028: RuntimeWarning: invalid value encountered in divide\n",
      "  return eXB/eXB.sum(1)[:,None]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm  # For detailed statistical summary\n",
    "from sklearn import datasets\n",
    "\n",
    "# Step 2: Load the Iris Dataset\n",
    "iris = datasets.load_iris()\n",
    "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y = pd.Series(iris.target, name='species')\n",
    "\n",
    "# Step 3: Split the Data (80% training, 20% testing)\n",
    "trainX, testX, trainY, testY = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Initialize and Train the Logistic Regression Model\n",
    "log_reg = LogisticRegression(solver='newton-cg', multi_class='multinomial', max_iter=200)\n",
    "log_reg.fit(trainX, trainY)\n",
    "\n",
    "# Step 5: Make Predictions\n",
    "y_pred = log_reg.predict(testX)\n",
    "\n",
    "# Step 6: Evaluate the Model\n",
    "print('Accuracy: {:.2f}'.format(accuracy_score(testY, y_pred)))\n",
    "print('Error rate: {:.2f}'.format(1 - accuracy_score(testY, y_pred)))\n",
    "\n",
    "# Step 7: Cross-Validation Scores\n",
    "clf = LogisticRegression(solver='newton-cg', multi_class='multinomial', max_iter=200)\n",
    "scores = cross_val_score(clf, trainX, trainY, cv=5)\n",
    "print(\"Cross-Validation Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "# Step 8: Confusion Matrix\n",
    "conf_matrix = confusion_matrix(testY, y_pred)\n",
    "print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "# Visualize the Confusion Matrix\n",
    "plt.matshow(conf_matrix, cmap=plt.cm.gray)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "# Step 9: Predicted Class Probabilities\n",
    "probability = log_reg.predict_proba(testX)\n",
    "print(\"\\nPredicted Probabilities:\\n\", probability)\n",
    "\n",
    "# Convert to DataFrame for Better Visualization\n",
    "df = pd.DataFrame(probability, columns=log_reg.classes_)\n",
    "\n",
    "# Step 10: Verify Probabilities Sum to 1\n",
    "df['sum'] = df.sum(axis=1)\n",
    "\n",
    "# Step 11: Add Predicted and Actual Classes\n",
    "df['predicted_class'] = y_pred\n",
    "df['actual_class'] = testY.reset_index(drop=True)\n",
    "\n",
    "# Step 12: Check if Predictions are Correct\n",
    "df['correct_prediction?'] = df['predicted_class'] == df['actual_class']\n",
    "\n",
    "# Step 13: Manually Calculate Accuracy\n",
    "true_predictions = df['correct_prediction?'].sum()\n",
    "total = df.shape[0]\n",
    "print('Manual Calculated Accuracy is: {:.2f}%'.format((true_predictions / total) * 100))\n",
    "\n",
    "# Step 14: Inspect Misclassified Instances\n",
    "wrong_pred = df[df[\"correct_prediction?\"] == False]\n",
    "print(\"\\nMisclassified Predictions:\\n\", wrong_pred)\n",
    "\n",
    "# Step 15: Multinomial Logit Model with Statsmodels\n",
    "# Add constant term for intercept\n",
    "X_sm = sm.add_constant(X)\n",
    "mnlogit_mod = sm.MNLogit(y, X_sm)\n",
    "mnlogit_fit = mnlogit_mod.fit(maxiter=100)\n",
    "print(mnlogit_fit.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6779d9-2d04-4b61-93b9-f1aa8e6ea4c7",
   "metadata": {},
   "source": [
    "<h1>Practical 4: Implement SVM classifier (Iris Dataset)</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d75497-11d5-4e41-b323-a47cb6c3293e",
   "metadata": {},
   "source": [
    "pip install numpy<br>\n",
    "pip install scikit-learn<br>\n",
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7f98998-2984-498d-aace-71cc2eb4bb2e",
   "metadata": {},
   "outputs": [
    {"text/plain": [
       "<Figure size 1000x800 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm, datasets\n",
    "\n",
    "# Step 2: Define helper function to create a mesh grid for plotting\n",
    "def make_meshgrid(x, y, h=0.02):\n",
    "    \"\"\"\n",
    "    Create a mesh of points to plot in the decision boundary\n",
    "    \n",
    "    Parameters:\n",
    "    - x: Data to base x-axis meshgrid on\n",
    "    - y: Data to base y-axis meshgrid on\n",
    "    - h: Step size for meshgrid, optional (default: 0.02)\n",
    "    \n",
    "    Returns:\n",
    "    - xx, yy: ndarray of grid points\n",
    "    \"\"\"\n",
    "    x_min, x_max = x.min() - 1, x.max() + 1\n",
    "    y_min, y_max = y.min() - 1, y.max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    return xx, yy\n",
    "# Step 3: Define helper function to plot decision boundaries\n",
    "def plot_contours(ax, clf, xx, yy, **params):\n",
    "    \"\"\"\n",
    "    Plot the decision boundaries for a classifier.\n",
    "    Parameters:\n",
    "    - ax: Matplotlib axes object\n",
    "    - clf: A classifier (SVM in this case)\n",
    "    - xx: Meshgrid ndarray for x-axis\n",
    "    - yy: Meshgrid ndarray for y-axis\n",
    "    - params: Optional parameters for contourf\n",
    "    \"\"\"\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    out = ax.contourf(xx, yy, Z, **params)\n",
    "    return out\n",
    "# Step 4: Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]   # Take only the first two features for easy visualization\n",
    "y = iris.target        # Target labels (0, 1, 2 for the three classes)\n",
    "\n",
    "# Step 5: Define SVM models with different kernels\n",
    "C = 1.0  # SVM regularization parameter\n",
    "models = (\n",
    "    svm.SVC(kernel=\"linear\", C=C),\n",
    "    svm.LinearSVC(C=C, max_iter=10000),\n",
    "    svm.SVC(kernel=\"rbf\", gamma=0.7, C=C),\n",
    "    svm.SVC(kernel=\"poly\", degree=3, gamma=\"auto\", C=C)\n",
    ")\n",
    "\n",
    "# Step 6: Train each model on the data\n",
    "models = [clf.fit(X, y) for clf in models]\n",
    "\n",
    "# Step 7: Titles for the subplots\n",
    "titles = (\n",
    "    \"SVC with linear kernel\",\n",
    "    \"LinearSVC (linear kernel)\",\n",
    "    \"SVC with RBF kernel\",\n",
    "    \"SVC with polynomial (degree 3) kernel\"\n",
    ")\n",
    "\n",
    "# Step 8: Plot the decision boundaries for each model\n",
    "fig, sub = plt.subplots(2, 2, figsize=(10, 8))\n",
    "plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "# Create the mesh grid for plotting\n",
    "X0, X1 = X[:, 0], X[:, 1]\n",
    "xx, yy = make_meshgrid(X0, X1)\n",
    "\n",
    "# Step 9: Loop through each model and plot\n",
    "for clf, title, ax in zip(models, titles, sub.flatten()):\n",
    "    plot_contours(ax, clf, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "    # Plot the data points\n",
    "    ax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=20, edgecolors=\"k\")\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xlabel(\"Sepal length\")\n",
    "    ax.set_ylabel(\"Sepal width\")\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Step 10: Display the final plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16e6258-8cf6-41b1-921e-5a98a1794e4c",
   "metadata": {},
   "source": [
    "<h1>Practical 5: Train and fine-tune a Decision Tree for the Moons Dataset</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c72cb1-bf6e-496f-ad99-c723325b4ef4",
   "metadata": {},
   "source": [
    "pip install numpy<br>\n",
    "pip install scikit-learn<br>\n",
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49b14514-b947-47f1-ae40-4d846ff878dd",
   "metadata": {},
   "outputs": [
    {
     "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'criterion': 'gini', 'max_leaf_nodes': 23, 'min_samples_split': 2}\n",
      "\n",
      "Training Scores and Parameters:\n",
      "0.7757 => {'criterion': 'gini', 'max_leaf_nodes': 2, 'min_samples_split': 2}\n",
      "0.7757 => {'criterion': 'gini', 'max_leaf_nodes': 2, 'min_samples_split': 3}\n",
      "0.7757 => {'criterion': 'gini', 'max_leaf_nodes': 2, 'min_samples_split': 4}\n",
      "0.8196 => {'criterion': 'gini', 'max_leaf_nodes': 3, 'min_samples_split': 2}\n",
      "0.8196 => {'criterion': 'gini', 'max_leaf_nodes': 3, 'min_samples_split': 3}\n",
      "0.8196 => {'criterion': 'gini', 'max_leaf_nodes': 3, 'min_samples_split': 4}\n",
      "0.8555 => {'criterion': 'gini', 'max_leaf_nodes': 4, 'min_samples_split': 2}\n",
      "0.8555 => {'criterion': 'gini', 'max_leaf_nodes': 4, 'min_samples_split': 3}\n",
      "0.8555 => {'criterion': 'gini', 'max_leaf_nodes': 4, 'min_samples_split': 4}\n",
      "0.8555 => {'criterion': 'gini', 'max_leaf_nodes': 5, 'min_samples_split': 2}\n",
      "0.8555 => {'criterion': 'gini', 'max_leaf_nodes': 5, 'min_samples_split': 3}\n",
      "0.8555 => {'criterion': 'gini', 'max_leaf_nodes': 5, 'min_samples_split': 4}\n",
      "0.8555 => {'criterion': 'gini', 'max_leaf_nodes': 6, 'min_samples_split': 2}\n",
      "0.8555 => {'criterion': 'gini', 'max_leaf_nodes': 6, 'min_samples_split': 3}\n",
      "0.8555 => {'criterion': 'gini', 'max_leaf_nodes': 6, 'min_samples_split': 4}\n",
      "0.8555 => {'criterion': 'gini', 'max_leaf_nodes': 7, 'min_samples_split': 2}\n",
      "0.8555 => {'criterion': 'gini', 'max_leaf_nodes': 7, 'min_samples_split': 3}\n",
      "0.8555 => {'criterion': 'gini', 'max_leaf_nodes': 7, 'min_samples_split': 4}\n",
      "0.8555 => {'criterion': 'gini', 'max_leaf_nodes': 8, 'min_samples_split': 2}\n",
      "0.8555 => {'criterion': 'gini', 'max_leaf_nodes': 8, 'min_samples_split': 3}\n",
      "0.8555 => {'criterion': 'gini', 'max_leaf_nodes': 8, 'min_samples_split': 4}\n",
      "0.8567 => {'criterion': 'gini', 'max_leaf_nodes': 9, 'min_samples_split': 2}\n",
      "0.8567 => {'criterion': 'gini', 'max_leaf_nodes': 9, 'min_samples_split': 3}\n",
      "0.8567 => {'criterion': 'gini', 'max_leaf_nodes': 9, 'min_samples_split': 4}\n",
      "0.8579 => {'criterion': 'gini', 'max_leaf_nodes': 10, 'min_samples_split': 2}\n",
      "0.8579 => {'criterion': 'gini', 'max_leaf_nodes': 10, 'min_samples_split': 3}\n",
      "0.8579 => {'criterion': 'gini', 'max_leaf_nodes': 10, 'min_samples_split': 4}\n",
      "0.8581 => {'criterion': 'gini', 'max_leaf_nodes': 11, 'min_samples_split': 2}\n",
      "0.8581 => {'criterion': 'gini', 'max_leaf_nodes': 11, 'min_samples_split': 3}\n",
      "0.8581 => {'criterion': 'gini', 'max_leaf_nodes': 11, 'min_samples_split': 4}\n",
      "0.8581 => {'criterion': 'gini', 'max_leaf_nodes': 12, 'min_samples_split': 2}\n",
      "0.8581 => {'criterion': 'gini', 'max_leaf_nodes': 12, 'min_samples_split': 3}\n",
      "0.8581 => {'criterion': 'gini', 'max_leaf_nodes': 12, 'min_samples_split': 4}\n",
      "0.8595 => {'criterion': 'gini', 'max_leaf_nodes': 13, 'min_samples_split': 2}\n",
      "0.8595 => {'criterion': 'gini', 'max_leaf_nodes': 13, 'min_samples_split': 3}\n",
      "0.8595 => {'criterion': 'gini', 'max_leaf_nodes': 13, 'min_samples_split': 4}\n",
      "0.8599 => {'criterion': 'gini', 'max_leaf_nodes': 14, 'min_samples_split': 2}\n",
      "0.8599 => {'criterion': 'gini', 'max_leaf_nodes': 14, 'min_samples_split': 3}\n",
      "0.8599 => {'criterion': 'gini', 'max_leaf_nodes': 14, 'min_samples_split': 4}\n",
      "0.8605 => {'criterion': 'gini', 'max_leaf_nodes': 15, 'min_samples_split': 2}\n",
      "0.8605 => {'criterion': 'gini', 'max_leaf_nodes': 15, 'min_samples_split': 3}\n",
      "0.8605 => {'criterion': 'gini', 'max_leaf_nodes': 15, 'min_samples_split': 4}\n",
      "0.8605 => {'criterion': 'gini', 'max_leaf_nodes': 16, 'min_samples_split': 2}\n",
      "0.8605 => {'criterion': 'gini', 'max_leaf_nodes': 16, 'min_samples_split': 3}\n",
      "0.8605 => {'criterion': 'gini', 'max_leaf_nodes': 16, 'min_samples_split': 4}\n",
      "0.8607 => {'criterion': 'gini', 'max_leaf_nodes': 17, 'min_samples_split': 2}\n",
      "0.8607 => {'criterion': 'gini', 'max_leaf_nodes': 17, 'min_samples_split': 3}\n",
      "0.8607 => {'criterion': 'gini', 'max_leaf_nodes': 17, 'min_samples_split': 4}\n",
      "0.8609 => {'criterion': 'gini', 'max_leaf_nodes': 18, 'min_samples_split': 2}\n",
      "0.8609 => {'criterion': 'gini', 'max_leaf_nodes': 18, 'min_samples_split': 3}\n",
      "0.8609 => {'criterion': 'gini', 'max_leaf_nodes': 18, 'min_samples_split': 4}\n",
      "0.8624 => {'criterion': 'gini', 'max_leaf_nodes': 19, 'min_samples_split': 2}\n",
      "0.8624 => {'criterion': 'gini', 'max_leaf_nodes': 19, 'min_samples_split': 3}\n",
      "0.8624 => {'criterion': 'gini', 'max_leaf_nodes': 19, 'min_samples_split': 4}\n",
      "0.8627 => {'criterion': 'gini', 'max_leaf_nodes': 20, 'min_samples_split': 2}\n",
      "0.8627 => {'criterion': 'gini', 'max_leaf_nodes': 20, 'min_samples_split': 3}\n",
      "0.8627 => {'criterion': 'gini', 'max_leaf_nodes': 20, 'min_samples_split': 4}\n",
      "0.8633 => {'criterion': 'gini', 'max_leaf_nodes': 21, 'min_samples_split': 2}\n",
      "0.8633 => {'criterion': 'gini', 'max_leaf_nodes': 21, 'min_samples_split': 3}\n",
      "0.8633 => {'criterion': 'gini', 'max_leaf_nodes': 21, 'min_samples_split': 4}\n",
      "0.8641 => {'criterion': 'gini', 'max_leaf_nodes': 22, 'min_samples_split': 2}\n",
      "0.8641 => {'criterion': 'gini', 'max_leaf_nodes': 22, 'min_samples_split': 3}\n",
      "0.8641 => {'criterion': 'gini', 'max_leaf_nodes': 22, 'min_samples_split': 4}\n",
      "0.8659 => {'criterion': 'gini', 'max_leaf_nodes': 23, 'min_samples_split': 2}\n",
      "0.8659 => {'criterion': 'gini', 'max_leaf_nodes': 23, 'min_samples_split': 3}\n",
      "0.8659 => {'criterion': 'gini', 'max_leaf_nodes': 23, 'min_samples_split': 4}\n",
      "0.8662 => {'criterion': 'gini', 'max_leaf_nodes': 24, 'min_samples_split': 2}\n",
      "0.8662 => {'criterion': 'gini', 'max_leaf_nodes': 24, 'min_samples_split': 3}\n",
      "0.8662 => {'criterion': 'gini', 'max_leaf_nodes': 24, 'min_samples_split': 4}\n",
      "0.8670 => {'criterion': 'gini', 'max_leaf_nodes': 25, 'min_samples_split': 2}\n",
      "0.8670 => {'criterion': 'gini', 'max_leaf_nodes': 25, 'min_samples_split': 3}\n",
      "0.8670 => {'criterion': 'gini', 'max_leaf_nodes': 25, 'min_samples_split': 4}\n",
      "0.8679 => {'criterion': 'gini', 'max_leaf_nodes': 26, 'min_samples_split': 2}\n",
      "0.8679 => {'criterion': 'gini', 'max_leaf_nodes': 26, 'min_samples_split': 3}\n",
      "0.8679 => {'criterion': 'gini', 'max_leaf_nodes': 26, 'min_samples_split': 4}\n",
      "0.8683 => {'criterion': 'gini', 'max_leaf_nodes': 27, 'min_samples_split': 2}\n",
      "0.8683 => {'criterion': 'gini', 'max_leaf_nodes': 27, 'min_samples_split': 3}\n",
      "0.8683 => {'criterion': 'gini', 'max_leaf_nodes': 27, 'min_samples_split': 4}\n",
      "0.8688 => {'criterion': 'gini', 'max_leaf_nodes': 28, 'min_samples_split': 2}\n",
      "0.8688 => {'criterion': 'gini', 'max_leaf_nodes': 28, 'min_samples_split': 3}\n",
      "0.8687 => {'criterion': 'gini', 'max_leaf_nodes': 28, 'min_samples_split': 4}\n",
      "0.8691 => {'criterion': 'gini', 'max_leaf_nodes': 29, 'min_samples_split': 2}\n",
      "0.8691 => {'criterion': 'gini', 'max_leaf_nodes': 29, 'min_samples_split': 3}\n",
      "0.8691 => {'criterion': 'gini', 'max_leaf_nodes': 29, 'min_samples_split': 4}\n",
      "0.8694 => {'criterion': 'gini', 'max_leaf_nodes': 30, 'min_samples_split': 2}\n",
      "0.8694 => {'criterion': 'gini', 'max_leaf_nodes': 30, 'min_samples_split': 3}\n",
      "0.8695 => {'criterion': 'gini', 'max_leaf_nodes': 30, 'min_samples_split': 4}\n",
      "0.8697 => {'criterion': 'gini', 'max_leaf_nodes': 31, 'min_samples_split': 2}\n",
      "0.8698 => {'criterion': 'gini', 'max_leaf_nodes': 31, 'min_samples_split': 3}\n",
      "0.8697 => {'criterion': 'gini', 'max_leaf_nodes': 31, 'min_samples_split': 4}\n",
      "0.8704 => {'criterion': 'gini', 'max_leaf_nodes': 32, 'min_samples_split': 2}\n",
      "0.8704 => {'criterion': 'gini', 'max_leaf_nodes': 32, 'min_samples_split': 3}\n",
      "0.8704 => {'criterion': 'gini', 'max_leaf_nodes': 32, 'min_samples_split': 4}\n",
      "0.8712 => {'criterion': 'gini', 'max_leaf_nodes': 33, 'min_samples_split': 2}\n",
      "0.8711 => {'criterion': 'gini', 'max_leaf_nodes': 33, 'min_samples_split': 3}\n",
      "0.8711 => {'criterion': 'gini', 'max_leaf_nodes': 33, 'min_samples_split': 4}\n",
      "0.8719 => {'criterion': 'gini', 'max_leaf_nodes': 34, 'min_samples_split': 2}\n",
      "0.8721 => {'criterion': 'gini', 'max_leaf_nodes': 34, 'min_samples_split': 3}\n",
      "0.8719 => {'criterion': 'gini', 'max_leaf_nodes': 34, 'min_samples_split': 4}\n",
      "0.8723 => {'criterion': 'gini', 'max_leaf_nodes': 35, 'min_samples_split': 2}\n",
      "0.8723 => {'criterion': 'gini', 'max_leaf_nodes': 35, 'min_samples_split': 3}\n",
      "0.8723 => {'criterion': 'gini', 'max_leaf_nodes': 35, 'min_samples_split': 4}\n",
      "0.8730 => {'criterion': 'gini', 'max_leaf_nodes': 36, 'min_samples_split': 2}\n",
      "0.8730 => {'criterion': 'gini', 'max_leaf_nodes': 36, 'min_samples_split': 3}\n",
      "0.8731 => {'criterion': 'gini', 'max_leaf_nodes': 36, 'min_samples_split': 4}\n",
      "0.8735 => {'criterion': 'gini', 'max_leaf_nodes': 37, 'min_samples_split': 2}\n",
      "0.8735 => {'criterion': 'gini', 'max_leaf_nodes': 37, 'min_samples_split': 3}\n",
      "0.8735 => {'criterion': 'gini', 'max_leaf_nodes': 37, 'min_samples_split': 4}\n",
      "0.8739 => {'criterion': 'gini', 'max_leaf_nodes': 38, 'min_samples_split': 2}\n",
      "0.8738 => {'criterion': 'gini', 'max_leaf_nodes': 38, 'min_samples_split': 3}\n",
      "0.8738 => {'criterion': 'gini', 'max_leaf_nodes': 38, 'min_samples_split': 4}\n",
      "0.8740 => {'criterion': 'gini', 'max_leaf_nodes': 39, 'min_samples_split': 2}\n",
      "0.8738 => {'criterion': 'gini', 'max_leaf_nodes': 39, 'min_samples_split': 3}\n",
      "0.8738 => {'criterion': 'gini', 'max_leaf_nodes': 39, 'min_samples_split': 4}\n",
      "0.8744 => {'criterion': 'gini', 'max_leaf_nodes': 40, 'min_samples_split': 2}\n",
      "0.8744 => {'criterion': 'gini', 'max_leaf_nodes': 40, 'min_samples_split': 3}\n",
      "0.8744 => {'criterion': 'gini', 'max_leaf_nodes': 40, 'min_samples_split': 4}\n",
      "0.8745 => {'criterion': 'gini', 'max_leaf_nodes': 41, 'min_samples_split': 2}\n",
      "0.8745 => {'criterion': 'gini', 'max_leaf_nodes': 41, 'min_samples_split': 3}\n",
      "0.8745 => {'criterion': 'gini', 'max_leaf_nodes': 41, 'min_samples_split': 4}\n",
      "0.8751 => {'criterion': 'gini', 'max_leaf_nodes': 42, 'min_samples_split': 2}\n",
      "0.8751 => {'criterion': 'gini', 'max_leaf_nodes': 42, 'min_samples_split': 3}\n",
      "0.8751 => {'criterion': 'gini', 'max_leaf_nodes': 42, 'min_samples_split': 4}\n",
      "0.8752 => {'criterion': 'gini', 'max_leaf_nodes': 43, 'min_samples_split': 2}\n",
      "0.8753 => {'criterion': 'gini', 'max_leaf_nodes': 43, 'min_samples_split': 3}\n",
      "0.8753 => {'criterion': 'gini', 'max_leaf_nodes': 43, 'min_samples_split': 4}\n",
      "0.8755 => {'criterion': 'gini', 'max_leaf_nodes': 44, 'min_samples_split': 2}\n",
      "0.8755 => {'criterion': 'gini', 'max_leaf_nodes': 44, 'min_samples_split': 3}\n",
      "0.8755 => {'criterion': 'gini', 'max_leaf_nodes': 44, 'min_samples_split': 4}\n",
      "0.8758 => {'criterion': 'gini', 'max_leaf_nodes': 45, 'min_samples_split': 2}\n",
      "0.8758 => {'criterion': 'gini', 'max_leaf_nodes': 45, 'min_samples_split': 3}\n",
      "0.8758 => {'criterion': 'gini', 'max_leaf_nodes': 45, 'min_samples_split': 4}\n",
      "0.8762 => {'criterion': 'gini', 'max_leaf_nodes': 46, 'min_samples_split': 2}\n",
      "0.8762 => {'criterion': 'gini', 'max_leaf_nodes': 46, 'min_samples_split': 3}\n",
      "0.8762 => {'criterion': 'gini', 'max_leaf_nodes': 46, 'min_samples_split': 4}\n",
      "0.8766 => {'criterion': 'gini', 'max_leaf_nodes': 47, 'min_samples_split': 2}\n",
      "0.8766 => {'criterion': 'gini', 'max_leaf_nodes': 47, 'min_samples_split': 3}\n",
      "0.8766 => {'criterion': 'gini', 'max_leaf_nodes': 47, 'min_samples_split': 4}\n",
      "0.8769 => {'criterion': 'gini', 'max_leaf_nodes': 48, 'min_samples_split': 2}\n",
      "0.8769 => {'criterion': 'gini', 'max_leaf_nodes': 48, 'min_samples_split': 3}\n",
      "0.8769 => {'criterion': 'gini', 'max_leaf_nodes': 48, 'min_samples_split': 4}\n",
      "0.8773 => {'criterion': 'gini', 'max_leaf_nodes': 49, 'min_samples_split': 2}\n",
      "0.8773 => {'criterion': 'gini', 'max_leaf_nodes': 49, 'min_samples_split': 3}\n",
      "0.8773 => {'criterion': 'gini', 'max_leaf_nodes': 49, 'min_samples_split': 4}\n",
      "0.7740 => {'criterion': 'entropy', 'max_leaf_nodes': 2, 'min_samples_split': 2}\n",
      "0.7740 => {'criterion': 'entropy', 'max_leaf_nodes': 2, 'min_samples_split': 3}\n",
      "0.7740 => {'criterion': 'entropy', 'max_leaf_nodes': 2, 'min_samples_split': 4}\n",
      "0.8234 => {'criterion': 'entropy', 'max_leaf_nodes': 3, 'min_samples_split': 2}\n",
      "0.8234 => {'criterion': 'entropy', 'max_leaf_nodes': 3, 'min_samples_split': 3}\n",
      "0.8234 => {'criterion': 'entropy', 'max_leaf_nodes': 3, 'min_samples_split': 4}\n",
      "0.8542 => {'criterion': 'entropy', 'max_leaf_nodes': 4, 'min_samples_split': 2}\n",
      "0.8542 => {'criterion': 'entropy', 'max_leaf_nodes': 4, 'min_samples_split': 3}\n",
      "0.8542 => {'criterion': 'entropy', 'max_leaf_nodes': 4, 'min_samples_split': 4}\n",
      "0.8542 => {'criterion': 'entropy', 'max_leaf_nodes': 5, 'min_samples_split': 2}\n",
      "0.8542 => {'criterion': 'entropy', 'max_leaf_nodes': 5, 'min_samples_split': 3}\n",
      "0.8542 => {'criterion': 'entropy', 'max_leaf_nodes': 5, 'min_samples_split': 4}\n",
      "0.8542 => {'criterion': 'entropy', 'max_leaf_nodes': 6, 'min_samples_split': 2}\n",
      "0.8542 => {'criterion': 'entropy', 'max_leaf_nodes': 6, 'min_samples_split': 3}\n",
      "0.8542 => {'criterion': 'entropy', 'max_leaf_nodes': 6, 'min_samples_split': 4}\n",
      "0.8542 => {'criterion': 'entropy', 'max_leaf_nodes': 7, 'min_samples_split': 2}\n",
      "0.8542 => {'criterion': 'entropy', 'max_leaf_nodes': 7, 'min_samples_split': 3}\n",
      "0.8542 => {'criterion': 'entropy', 'max_leaf_nodes': 7, 'min_samples_split': 4}\n",
      "0.8542 => {'criterion': 'entropy', 'max_leaf_nodes': 8, 'min_samples_split': 2}\n",
      "0.8542 => {'criterion': 'entropy', 'max_leaf_nodes': 8, 'min_samples_split': 3}\n",
      "0.8542 => {'criterion': 'entropy', 'max_leaf_nodes': 8, 'min_samples_split': 4}\n",
      "0.8542 => {'criterion': 'entropy', 'max_leaf_nodes': 9, 'min_samples_split': 2}\n",
      "0.8542 => {'criterion': 'entropy', 'max_leaf_nodes': 9, 'min_samples_split': 3}\n",
      "0.8542 => {'criterion': 'entropy', 'max_leaf_nodes': 9, 'min_samples_split': 4}\n",
      "0.8542 => {'criterion': 'entropy', 'max_leaf_nodes': 10, 'min_samples_split': 2}\n",
      "0.8542 => {'criterion': 'entropy', 'max_leaf_nodes': 10, 'min_samples_split': 3}\n",
      "0.8542 => {'criterion': 'entropy', 'max_leaf_nodes': 10, 'min_samples_split': 4}\n",
      "0.8542 => {'criterion': 'entropy', 'max_leaf_nodes': 11, 'min_samples_split': 2}\n",
      "0.8542 => {'criterion': 'entropy', 'max_leaf_nodes': 11, 'min_samples_split': 3}\n",
      "0.8542 => {'criterion': 'entropy', 'max_leaf_nodes': 11, 'min_samples_split': 4}\n",
      "0.8550 => {'criterion': 'entropy', 'max_leaf_nodes': 12, 'min_samples_split': 2}\n",
      "0.8550 => {'criterion': 'entropy', 'max_leaf_nodes': 12, 'min_samples_split': 3}\n",
      "0.8550 => {'criterion': 'entropy', 'max_leaf_nodes': 12, 'min_samples_split': 4}\n",
      "0.8550 => {'criterion': 'entropy', 'max_leaf_nodes': 13, 'min_samples_split': 2}\n",
      "0.8550 => {'criterion': 'entropy', 'max_leaf_nodes': 13, 'min_samples_split': 3}\n",
      "0.8550 => {'criterion': 'entropy', 'max_leaf_nodes': 13, 'min_samples_split': 4}\n",
      "0.8550 => {'criterion': 'entropy', 'max_leaf_nodes': 14, 'min_samples_split': 2}\n",
      "0.8550 => {'criterion': 'entropy', 'max_leaf_nodes': 14, 'min_samples_split': 3}\n",
      "0.8550 => {'criterion': 'entropy', 'max_leaf_nodes': 14, 'min_samples_split': 4}\n",
      "0.8550 => {'criterion': 'entropy', 'max_leaf_nodes': 15, 'min_samples_split': 2}\n",
      "0.8550 => {'criterion': 'entropy', 'max_leaf_nodes': 15, 'min_samples_split': 3}\n",
      "0.8550 => {'criterion': 'entropy', 'max_leaf_nodes': 15, 'min_samples_split': 4}\n",
      "0.8550 => {'criterion': 'entropy', 'max_leaf_nodes': 16, 'min_samples_split': 2}\n",
      "0.8550 => {'criterion': 'entropy', 'max_leaf_nodes': 16, 'min_samples_split': 3}\n",
      "0.8550 => {'criterion': 'entropy', 'max_leaf_nodes': 16, 'min_samples_split': 4}\n",
      "0.8550 => {'criterion': 'entropy', 'max_leaf_nodes': 17, 'min_samples_split': 2}\n",
      "0.8550 => {'criterion': 'entropy', 'max_leaf_nodes': 17, 'min_samples_split': 3}\n",
      "0.8550 => {'criterion': 'entropy', 'max_leaf_nodes': 17, 'min_samples_split': 4}\n",
      "0.8589 => {'criterion': 'entropy', 'max_leaf_nodes': 18, 'min_samples_split': 2}\n",
      "0.8589 => {'criterion': 'entropy', 'max_leaf_nodes': 18, 'min_samples_split': 3}\n",
      "0.8589 => {'criterion': 'entropy', 'max_leaf_nodes': 18, 'min_samples_split': 4}\n",
      "0.8597 => {'criterion': 'entropy', 'max_leaf_nodes': 19, 'min_samples_split': 2}\n",
      "0.8597 => {'criterion': 'entropy', 'max_leaf_nodes': 19, 'min_samples_split': 3}\n",
      "0.8597 => {'criterion': 'entropy', 'max_leaf_nodes': 19, 'min_samples_split': 4}\n",
      "0.8601 => {'criterion': 'entropy', 'max_leaf_nodes': 20, 'min_samples_split': 2}\n",
      "0.8601 => {'criterion': 'entropy', 'max_leaf_nodes': 20, 'min_samples_split': 3}\n",
      "0.8601 => {'criterion': 'entropy', 'max_leaf_nodes': 20, 'min_samples_split': 4}\n",
      "0.8601 => {'criterion': 'entropy', 'max_leaf_nodes': 21, 'min_samples_split': 2}\n",
      "0.8601 => {'criterion': 'entropy', 'max_leaf_nodes': 21, 'min_samples_split': 3}\n",
      "0.8601 => {'criterion': 'entropy', 'max_leaf_nodes': 21, 'min_samples_split': 4}\n",
      "0.8613 => {'criterion': 'entropy', 'max_leaf_nodes': 22, 'min_samples_split': 2}\n",
      "0.8613 => {'criterion': 'entropy', 'max_leaf_nodes': 22, 'min_samples_split': 3}\n",
      "0.8613 => {'criterion': 'entropy', 'max_leaf_nodes': 22, 'min_samples_split': 4}\n",
      "0.8616 => {'criterion': 'entropy', 'max_leaf_nodes': 23, 'min_samples_split': 2}\n",
      "0.8616 => {'criterion': 'entropy', 'max_leaf_nodes': 23, 'min_samples_split': 3}\n",
      "0.8616 => {'criterion': 'entropy', 'max_leaf_nodes': 23, 'min_samples_split': 4}\n",
      "0.8620 => {'criterion': 'entropy', 'max_leaf_nodes': 24, 'min_samples_split': 2}\n",
      "0.8620 => {'criterion': 'entropy', 'max_leaf_nodes': 24, 'min_samples_split': 3}\n",
      "0.8620 => {'criterion': 'entropy', 'max_leaf_nodes': 24, 'min_samples_split': 4}\n",
      "0.8620 => {'criterion': 'entropy', 'max_leaf_nodes': 25, 'min_samples_split': 2}\n",
      "0.8620 => {'criterion': 'entropy', 'max_leaf_nodes': 25, 'min_samples_split': 3}\n",
      "0.8620 => {'criterion': 'entropy', 'max_leaf_nodes': 25, 'min_samples_split': 4}\n",
      "0.8622 => {'criterion': 'entropy', 'max_leaf_nodes': 26, 'min_samples_split': 2}\n",
      "0.8622 => {'criterion': 'entropy', 'max_leaf_nodes': 26, 'min_samples_split': 3}\n",
      "0.8622 => {'criterion': 'entropy', 'max_leaf_nodes': 26, 'min_samples_split': 4}\n",
      "0.8628 => {'criterion': 'entropy', 'max_leaf_nodes': 27, 'min_samples_split': 2}\n",
      "0.8628 => {'criterion': 'entropy', 'max_leaf_nodes': 27, 'min_samples_split': 3}\n",
      "0.8628 => {'criterion': 'entropy', 'max_leaf_nodes': 27, 'min_samples_split': 4}\n",
      "0.8628 => {'criterion': 'entropy', 'max_leaf_nodes': 28, 'min_samples_split': 2}\n",
      "0.8628 => {'criterion': 'entropy', 'max_leaf_nodes': 28, 'min_samples_split': 3}\n",
      "0.8628 => {'criterion': 'entropy', 'max_leaf_nodes': 28, 'min_samples_split': 4}\n",
      "0.8628 => {'criterion': 'entropy', 'max_leaf_nodes': 29, 'min_samples_split': 2}\n",
      "0.8628 => {'criterion': 'entropy', 'max_leaf_nodes': 29, 'min_samples_split': 3}\n",
      "0.8628 => {'criterion': 'entropy', 'max_leaf_nodes': 29, 'min_samples_split': 4}\n",
      "0.8631 => {'criterion': 'entropy', 'max_leaf_nodes': 30, 'min_samples_split': 2}\n",
      "0.8631 => {'criterion': 'entropy', 'max_leaf_nodes': 30, 'min_samples_split': 3}\n",
      "0.8631 => {'criterion': 'entropy', 'max_leaf_nodes': 30, 'min_samples_split': 4}\n",
      "0.8637 => {'criterion': 'entropy', 'max_leaf_nodes': 31, 'min_samples_split': 2}\n",
      "0.8637 => {'criterion': 'entropy', 'max_leaf_nodes': 31, 'min_samples_split': 3}\n",
      "0.8637 => {'criterion': 'entropy', 'max_leaf_nodes': 31, 'min_samples_split': 4}\n",
      "0.8638 => {'criterion': 'entropy', 'max_leaf_nodes': 32, 'min_samples_split': 2}\n",
      "0.8638 => {'criterion': 'entropy', 'max_leaf_nodes': 32, 'min_samples_split': 3}\n",
      "0.8638 => {'criterion': 'entropy', 'max_leaf_nodes': 32, 'min_samples_split': 4}\n",
      "0.8645 => {'criterion': 'entropy', 'max_leaf_nodes': 33, 'min_samples_split': 2}\n",
      "0.8645 => {'criterion': 'entropy', 'max_leaf_nodes': 33, 'min_samples_split': 3}\n",
      "0.8645 => {'criterion': 'entropy', 'max_leaf_nodes': 33, 'min_samples_split': 4}\n",
      "0.8646 => {'criterion': 'entropy', 'max_leaf_nodes': 34, 'min_samples_split': 2}\n",
      "0.8646 => {'criterion': 'entropy', 'max_leaf_nodes': 34, 'min_samples_split': 3}\n",
      "0.8646 => {'criterion': 'entropy', 'max_leaf_nodes': 34, 'min_samples_split': 4}\n",
      "0.8646 => {'criterion': 'entropy', 'max_leaf_nodes': 35, 'min_samples_split': 2}\n",
      "0.8646 => {'criterion': 'entropy', 'max_leaf_nodes': 35, 'min_samples_split': 3}\n",
      "0.8646 => {'criterion': 'entropy', 'max_leaf_nodes': 35, 'min_samples_split': 4}\n",
      "0.8646 => {'criterion': 'entropy', 'max_leaf_nodes': 36, 'min_samples_split': 2}\n",
      "0.8646 => {'criterion': 'entropy', 'max_leaf_nodes': 36, 'min_samples_split': 3}\n",
      "0.8646 => {'criterion': 'entropy', 'max_leaf_nodes': 36, 'min_samples_split': 4}\n",
      "0.8647 => {'criterion': 'entropy', 'max_leaf_nodes': 37, 'min_samples_split': 2}\n",
      "0.8647 => {'criterion': 'entropy', 'max_leaf_nodes': 37, 'min_samples_split': 3}\n",
      "0.8647 => {'criterion': 'entropy', 'max_leaf_nodes': 37, 'min_samples_split': 4}\n",
      "0.8648 => {'criterion': 'entropy', 'max_leaf_nodes': 38, 'min_samples_split': 2}\n",
      "0.8648 => {'criterion': 'entropy', 'max_leaf_nodes': 38, 'min_samples_split': 3}\n",
      "0.8648 => {'criterion': 'entropy', 'max_leaf_nodes': 38, 'min_samples_split': 4}\n",
      "0.8651 => {'criterion': 'entropy', 'max_leaf_nodes': 39, 'min_samples_split': 2}\n",
      "0.8651 => {'criterion': 'entropy', 'max_leaf_nodes': 39, 'min_samples_split': 3}\n",
      "0.8651 => {'criterion': 'entropy', 'max_leaf_nodes': 39, 'min_samples_split': 4}\n",
      "0.8651 => {'criterion': 'entropy', 'max_leaf_nodes': 40, 'min_samples_split': 2}\n",
      "0.8651 => {'criterion': 'entropy', 'max_leaf_nodes': 40, 'min_samples_split': 3}\n",
      "0.8651 => {'criterion': 'entropy', 'max_leaf_nodes': 40, 'min_samples_split': 4}\n",
      "0.8662 => {'criterion': 'entropy', 'max_leaf_nodes': 41, 'min_samples_split': 2}\n",
      "0.8662 => {'criterion': 'entropy', 'max_leaf_nodes': 41, 'min_samples_split': 3}\n",
      "0.8662 => {'criterion': 'entropy', 'max_leaf_nodes': 41, 'min_samples_split': 4}\n",
      "0.8668 => {'criterion': 'entropy', 'max_leaf_nodes': 42, 'min_samples_split': 2}\n",
      "0.8668 => {'criterion': 'entropy', 'max_leaf_nodes': 42, 'min_samples_split': 3}\n",
      "0.8668 => {'criterion': 'entropy', 'max_leaf_nodes': 42, 'min_samples_split': 4}\n",
      "0.8671 => {'criterion': 'entropy', 'max_leaf_nodes': 43, 'min_samples_split': 2}\n",
      "0.8671 => {'criterion': 'entropy', 'max_leaf_nodes': 43, 'min_samples_split': 3}\n",
      "0.8671 => {'criterion': 'entropy', 'max_leaf_nodes': 43, 'min_samples_split': 4}\n",
      "0.8679 => {'criterion': 'entropy', 'max_leaf_nodes': 44, 'min_samples_split': 2}\n",
      "0.8679 => {'criterion': 'entropy', 'max_leaf_nodes': 44, 'min_samples_split': 3}\n",
      "0.8679 => {'criterion': 'entropy', 'max_leaf_nodes': 44, 'min_samples_split': 4}\n",
      "0.8682 => {'criterion': 'entropy', 'max_leaf_nodes': 45, 'min_samples_split': 2}\n",
      "0.8682 => {'criterion': 'entropy', 'max_leaf_nodes': 45, 'min_samples_split': 3}\n",
      "0.8682 => {'criterion': 'entropy', 'max_leaf_nodes': 45, 'min_samples_split': 4}\n",
      "0.8691 => {'criterion': 'entropy', 'max_leaf_nodes': 46, 'min_samples_split': 2}\n",
      "0.8691 => {'criterion': 'entropy', 'max_leaf_nodes': 46, 'min_samples_split': 3}\n",
      "0.8691 => {'criterion': 'entropy', 'max_leaf_nodes': 46, 'min_samples_split': 4}\n",
      "0.8693 => {'criterion': 'entropy', 'max_leaf_nodes': 47, 'min_samples_split': 2}\n",
      "0.8693 => {'criterion': 'entropy', 'max_leaf_nodes': 47, 'min_samples_split': 3}\n",
      "0.8694 => {'criterion': 'entropy', 'max_leaf_nodes': 47, 'min_samples_split': 4}\n",
      "0.8698 => {'criterion': 'entropy', 'max_leaf_nodes': 48, 'min_samples_split': 2}\n",
      "0.8698 => {'criterion': 'entropy', 'max_leaf_nodes': 48, 'min_samples_split': 3}\n",
      "0.8698 => {'criterion': 'entropy', 'max_leaf_nodes': 48, 'min_samples_split': 4}\n",
      "0.8698 => {'criterion': 'entropy', 'max_leaf_nodes': 49, 'min_samples_split': 2}\n",
      "0.8698 => {'criterion': 'entropy', 'max_leaf_nodes': 49, 'min_samples_split': 3}\n",
      "0.8698 => {'criterion': 'entropy', 'max_leaf_nodes': 49, 'min_samples_split': 4}\n",
      "\n",
      "Training Accuracy: 86.28 %\n",
      "\n",
      "Confusion Matrix (Train):\n",
      "[[3560  416]\n",
      " [ 682 3342]]\n",
      "\n",
      "Precision: 88.93%\n",
      "Recall: 83.05%\n",
      "F1 Score: 85.89%\n",
      "\n",
      "Test Accuracy: 87.3 %\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    ")\n",
    "\n",
    "# Function to plot dataset\n",
    "def plot_dataset(X, y, axes):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(X[:, 0][y == 0], X[:, 1][y == 0], \"bs\", alpha=0.5)\n",
    "    plt.plot(X[:, 0][y == 1], X[:, 1][y == 1], \"g^\", alpha=0.2)\n",
    "    plt.axis(axes)\n",
    "    plt.grid(True)\n",
    "    plt.xlabel(r\"$x_1$\", fontsize=20)\n",
    "    plt.ylabel(r\"$x_2$\", fontsize=20)\n",
    "    plt.show()\n",
    "\n",
    "# Generate toy dataset\n",
    "X, y = make_moons(n_samples=10000, noise=0.4, random_state=21)\n",
    "plot_dataset(X, y, [-3, 5, -3, 3])\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Define Decision Tree and parameter grid\n",
    "tree_clf = DecisionTreeClassifier()\n",
    "parameter = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_leaf_nodes': list(range(2, 50)),\n",
    "    'min_samples_split': [2, 3, 4],\n",
    "}\n",
    "\n",
    "# Apply Grid Search with cross-validation\n",
    "clf = GridSearchCV(tree_clf, parameter, cv=5, scoring=\"accuracy\", return_train_score=True, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Output best parameters\n",
    "print(\"Best Parameters:\", clf.best_params_)\n",
    "\n",
    "# Training scores per parameter combo\n",
    "print(\"\\nTraining Scores and Parameters:\")\n",
    "cvres = clf.cv_results_\n",
    "for mean_score, params in zip(cvres[\"mean_train_score\"], cvres[\"params\"]):\n",
    "    print(f\"{mean_score:.4f} => {params}\")\n",
    "\n",
    "# Evaluate training accuracy\n",
    "train_accuracy = clf.score(X_train, y_train)\n",
    "print(\"\\nTraining Accuracy:\", round(train_accuracy*100,2),\"%\")\n",
    "\n",
    "# Confusion matrix\n",
    "pred = clf.predict(X_train)\n",
    "cm = confusion_matrix(y_train, pred)\n",
    "print(\"\\nConfusion Matrix (Train):\")\n",
    "print(cm)\n",
    "\n",
    "# Precision, recall, F1-score\n",
    "precision = precision_score(y_train, pred)\n",
    "recall = recall_score(y_train, pred)\n",
    "f1 = f1_score(y_train, pred)\n",
    "print(f\"\\nPrecision: {round(precision*100,2)}%\")\n",
    "print(f\"Recall: {round(recall*100,2)}%\")\n",
    "print(f\"F1 Score: {round(f1*100,2)}%\")\n",
    "\n",
    "# Test accuracy\n",
    "test_accuracy = clf.score(X_test, y_test)\n",
    "print(\"\\nTest Accuracy:\", round(test_accuracy*100,2),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219eecb8-490c-4b75-a624-f80efc6d1450",
   "metadata": {},
   "source": [
    "<h1>Practical 6: Implement Multi-Layer Perceptron for the classification of handwritten digits (MNIST Dataset)</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc119c1-555e-4b81-9ac9-aede9043c2fa",
   "metadata": {},
   "source": [
    "pip install numpy<br>\n",
    "pip install torchvision<br>\n",
    "pip install torch<br>\n",
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fab828c-558d-4e6d-978d-dbd221ca6bfd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 3021.1651\n",
      "Epoch 2/10, Loss: 2887.7825\n",
      "Epoch 3/10, Loss: 2861.5363\n",
      "Epoch 4/10, Loss: 2853.1553\n",
      "Epoch 5/10, Loss: 2842.8877\n",
      "Epoch 6/10, Loss: 2835.8472\n",
      "Epoch 7/10, Loss: 2831.1528\n",
      "Epoch 8/10, Loss: 2827.8408\n",
      "Epoch 9/10, Loss: 2825.6001\n",
      "Epoch 10/10, Loss: 2823.2832\n",
      "\n",
      "Test Accuracy: 95.50%\n"
     ]
    },
    {
     "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load and normalize MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset  = datasets.MNIST(root='./data', train=False, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader  = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Build the neural network model\n",
    "class MNISTModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTModel, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.softmax(self.fc3(x), dim=1)\n",
    "        return x\n",
    "\n",
    "model = MNISTModel().to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "# Evaluate accuracy\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"\\nTest Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "# Display predictions for first 10 test images\n",
    "examples = iter(test_loader)\n",
    "example_data, example_targets = next(examples)\n",
    "\n",
    "for i in range(10):\n",
    "    img = example_data[i][0]\n",
    "    label = example_targets[i].item()\n",
    "    output = model(img.unsqueeze(0).to(device))\n",
    "    pred = torch.argmax(output).item()\n",
    "    plt.imshow(img.squeeze(), cmap='gray')\n",
    "    plt.title(f\"Predicted: {pred}, True: {label}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cc76ab-e819-46e8-9664-99e41b08a1db",
   "metadata": {},
   "source": [
    "<h1>Practical 7: Classification of images of clothing using Tensorflow (Fashion MNIST dataset).</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af13287-2e9a-48a4-83ab-5a8d3c639fc9",
   "metadata": {},
   "source": [
    "pip install numpy<br>\n",
    "pip install torchvision<br>\n",
    "pip install torch<br>\n",
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd60f329-a005-489e-a088-cf4c6bc6e066",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "text/plain": [
       "<Figure size 1500x1500 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Acc = 82.43%, Validation Acc = 83.08%\n",
      "Epoch 2: Training Acc = 86.41%, Validation Acc = 85.83%\n",
      "Epoch 3: Training Acc = 87.71%, Validation Acc = 86.31%\n",
      "Epoch 4: Training Acc = 88.61%, Validation Acc = 86.73%\n",
      "Epoch 5: Training Acc = 89.33%, Validation Acc = 87.35%\n",
      "Epoch 6: Training Acc = 89.89%, Validation Acc = 88.32%\n",
      "Epoch 7: Training Acc = 90.21%, Validation Acc = 87.70%\n",
      "Epoch 8: Training Acc = 90.84%, Validation Acc = 88.38%\n",
      "Epoch 9: Training Acc = 91.21%, Validation Acc = 87.82%\n",
      "Epoch 10: Training Acc = 91.55%, Validation Acc = 87.69%\n"
     ]
    },
    {
     "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "text/plain": [
       "<Figure size 1500x1500 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Class names for Fashion MNIST\n",
    "class_names = [\n",
    "    \"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "    \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"\n",
    "]\n",
    "\n",
    "# Load and transform the Fashion MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Visualize first 10 training samples\n",
    "plt.figure(figsize=(15, 15))\n",
    "for i in range(10):\n",
    "    image, label = train_dataset[i]\n",
    "    plt.subplot(1, 10, i+1)\n",
    "    plt.imshow(image.squeeze(), cmap=\"binary\")\n",
    "    plt.title(class_names[label], fontsize=10)\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Define the neural network model\n",
    "class FashionNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FashionNet, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28*28, 300),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(300, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "model = FashionNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "train_acc_history = []\n",
    "val_acc_history = []\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_acc = 100 * correct / total\n",
    "    train_acc_history.append(train_acc)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_acc = 100 * correct / total\n",
    "    val_acc_history.append(val_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Training Acc = {train_acc:.2f}%, Validation Acc = {val_acc:.2f}%\")\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.plot(train_acc_history, label='Training Accuracy')\n",
    "plt.plot(val_acc_history, label='Validation Accuracy')\n",
    "plt.title('Training vs Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Predict and visualize 10 test samples\n",
    "samples = next(iter(test_loader))\n",
    "images, labels = samples\n",
    "images = images.to(device)\n",
    "outputs = model(images[:10])\n",
    "preds = torch.argmax(outputs, axis=1).cpu().numpy()\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "for i in range(10):\n",
    "    plt.subplot(1, 10, i+1)\n",
    "    plt.imshow(images[i].cpu().squeeze(), cmap=\"binary\")\n",
    "    plt.title(f\"{class_names[preds[i]]}\", fontsize=10)\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a844b37b-a2c5-4a59-bed6-35b16aa95ada",
   "metadata": {},
   "source": [
    "<h1>Practical 8: Train an SVM regressor on the California Housing Dataset</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27213370-e084-40d4-b5c7-227d2355bd96",
   "metadata": {},
   "source": [
    "pip install numpy<br>\n",
    "pip install seaborn<br>\n",
    "pip install pandas<br>\n",
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37d0db08-2391-42da-9f51-59962ef2ac87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17000 entries, 0 to 16999\n",
      "Data columns (total 9 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   longitude           17000 non-null  float64\n",
      " 1   latitude            17000 non-null  float64\n",
      " 2   housing_median_age  17000 non-null  float64\n",
      " 3   total_rooms         17000 non-null  float64\n",
      " 4   total_bedrooms      17000 non-null  float64\n",
      " 5   population          17000 non-null  float64\n",
      " 6   households          17000 non-null  float64\n",
      " 7   median_income       17000 non-null  float64\n",
      " 8   median_house_value  17000 non-null  float64\n",
      "dtypes: float64(9)\n",
      "memory usage: 1.2 MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3000 entries, 0 to 2999\n",
      "Data columns (total 9 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   longitude           3000 non-null   float64\n",
      " 1   latitude            3000 non-null   float64\n",
      " 2   housing_median_age  3000 non-null   float64\n",
      " 3   total_rooms         3000 non-null   float64\n",
      " 4   total_bedrooms      3000 non-null   float64\n",
      " 5   population          3000 non-null   float64\n",
      " 6   households          3000 non-null   float64\n",
      " 7   median_income       3000 non-null   float64\n",
      " 8   median_house_value  3000 non-null   float64\n",
      "dtypes: float64(9)\n",
      "memory usage: 211.1 KB\n",
      "None\n"
     ]
    },
    {
     "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1513: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=vector, **plot_kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1513: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=vector, **plot_kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1513: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=vector, **plot_kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1513: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=vector, **plot_kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1513: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=vector, **plot_kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1513: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=vector, **plot_kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1513: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=vector, **plot_kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1513: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=vector, **plot_kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1513: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=vector, **plot_kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n",
      "c:\\users\\bilal hasan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\axisgrid.py:1615: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.\n",
      "  func(x=x, y=y, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20000 entries, 0 to 19999\n",
      "Data columns (total 6 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   total_rooms         20000 non-null  float64\n",
      " 1   total_bedrooms      20000 non-null  float64\n",
      " 2   housing_median_age  20000 non-null  float64\n",
      " 3   median_income       20000 non-null  float64\n",
      " 4   population          20000 non-null  float64\n",
      " 5   households          20000 non-null  float64\n",
      "dtypes: float64(6)\n",
      "memory usage: 937.6 KB\n",
      "   Real Values  Predicted Values\n",
      "0     142700.0     134284.806553\n",
      "1     500001.0     464932.991647\n",
      "2      61800.0      89125.353865\n",
      "3     162800.0     134657.075099\n",
      "4      90600.0     113504.709188\n"
     ]
    },
    {
     "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
      "text/plain": [
       "<Figure size 2250x2250 with 90 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1) IMPORT LIBRARIES\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# 2) LOAD DATA\n",
    "test  = pd.read_csv(\"california_housing_test.csv\")\n",
    "train = pd.read_csv(\"california_housing_train.csv\")\n",
    "train.head()\n",
    "train.tail()\n",
    "print(train.info())\n",
    "print(test.info())\n",
    "# 3) SAVE TARGET & COMBINE FOR PREPROCESSING\n",
    "n_train = train.shape[0]\n",
    "n_test  = test.shape[0]\n",
    "y       = train['median_house_value'].values\n",
    "data = pd.concat((train, test)).reset_index(drop=True)\n",
    "data.drop(['longitude', 'latitude'], axis=1, inplace=True)\n",
    "# 4) OPTIONAL: QUICK VISUALIZATIONS\n",
    "plt.figure()\n",
    "sns.heatmap(data.corr(), cmap='coolwarm')\n",
    "plt.show()\n",
    "sns.lmplot(x='median_income',      y='median_house_value', data=train)\n",
    "sns.lmplot(x='housing_median_age', y='median_house_value', data=train)\n",
    "sns.pairplot(train, palette='rainbow')\n",
    "# 5) FEATURE SELECTION & MISSING-VALUE IMPUTATION\n",
    "data = data[['total_rooms', 'total_bedrooms',\n",
    "             'housing_median_age', 'median_income',\n",
    "             'population', 'households']]\n",
    "data.info()\n",
    "for col in data.columns:\n",
    "    data.fillna(data.mean(), inplace=True)\n",
    "# split back into train / test\n",
    "train = data[:n_train]\n",
    "test  = data[n_train:]\n",
    "# 6) TRAIN/VALIDATION SPLIT\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train, y, test_size=0.2, random_state=42\n",
    ")\n",
    "# 7) SCALE FEATURES & TARGET\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test  = sc_X.transform(X_test)    # <-- use transform, not fit_transform\n",
    "sc_y    = StandardScaler()\n",
    "# keep y_train/y_test 1D for sklearn, but scale them as column-vectors then ravel\n",
    "y_train = sc_y.fit_transform(y_train.reshape(-1,1)).ravel()\n",
    "y_test  = sc_y.transform(   y_test.reshape(-1,1)).ravel()\n",
    "# 8) FIT THE SVR MODEL\n",
    "from sklearn.svm import SVR\n",
    "regressor = SVR(kernel='rbf')\n",
    "regressor.fit(X_train, y_train)     # no more DataConversionWarning\n",
    "# 9) PREDICT & INVERSE-SCALE THE OUTPUT\n",
    "y_pred = regressor.predict(X_test)  \n",
    "# bring back to original scale\n",
    "y_pred = sc_y.inverse_transform(y_pred.reshape(-1,1)).flatten()\n",
    "# 10) COMPARE REAL VS. PREDICTED\n",
    "df = pd.DataFrame({\n",
    "    'Real Values'     : sc_y.inverse_transform(y_test.reshape(-1,1)).flatten(),\n",
    "    'Predicted Values': y_pred\n",
    "})\n",
    "print(df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
