{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "783937ab-d578-40d0-afab-13ce5765624a",
   "metadata": {},
   "source": [
    "<h1> Practical No. 1: Write a program to implement sentence segmentation and word tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42e737c4-5603-47c4-8ecd-f74903cb9920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokens: ['My', 'Name', 'is', 'Bilal', 'and', 'I', 'am', 'Learning', 'NLP', 'segmentation', 'using', 'Tokenization', '!', 'Which', 'help', 'me', 'to', 'learn', 'to', 'train', 'machine', 'for', 'my', 'models']\n",
      "Sentence Tokens: ['My Name is Bilal and I am Learning NLP segmentation using Tokenization!', 'Which help me to learn to train machine for my models']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\BILAL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Sample text\n",
    "text = \"My Name is Bilal and I am Learning NLP segmentation using Tokenization! Which help me to learn to train machine for my models\"\n",
    "\n",
    "# Word Tokenization \n",
    "word_tokens = word_tokenize(text)\n",
    "print(\"Word Tokens:\", word_tokens)\n",
    "\n",
    "# Sentence Tokenization \n",
    "sentence_tokens = sent_tokenize(text)\n",
    "print(\"Sentence Tokens:\", sentence_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dfe092-4d75-4ac0-a949-2d1142e802f6",
   "metadata": {},
   "source": [
    "<h1> Practical No. 2(A): Write a program to Implement stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19fc7901-7a54-4e9f-b636-056a892bc021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter Stemmer\n",
      "run  — — -> run\n",
      "runner  — — -> runner\n",
      "running  — — -> run\n",
      "ran  — — -> ran\n",
      "runs  — — -> run\n",
      "easily  — — -> easili\n",
      "caring  — — -> care\n",
      "history  — — -> histori\n",
      "historical  — — -> histor\n",
      "Snowball Stemmer\n",
      "run — — -> run\n",
      "runner — — -> runner\n",
      "running — — -> run\n",
      "ran — — -> ran\n",
      "runs — — -> run\n",
      "easily — — -> easili\n",
      "caring — — -> care\n",
      "history — — -> histori\n",
      "historical — — -> histor\n",
      "Lancaster Stemmer\n",
      "run  — — -> run\n",
      "runner  — — -> run\n",
      "running  — — -> run\n",
      "ran  — — -> ran\n",
      "runs  — — -> run\n",
      "easily  — — -> easy\n",
      "caring  — — -> car\n",
      "history  — — -> hist\n",
      "historical  — — -> hist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\BILAL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "words=['run','runner','running','ran','runs','easily','caring','history','historical']\n",
    "ps=PorterStemmer()\n",
    "print(\"Porter Stemmer\")\n",
    "for word in words:\n",
    "    print(word,' — — ->',ps.stem(word))\n",
    "\n",
    "snowball=SnowballStemmer(language='english')\n",
    "print(\"Snowball Stemmer\")\n",
    "for word in words:\n",
    "    print(word,'— — ->',snowball.stem(word))\n",
    "\n",
    "lancaster=LancasterStemmer()\n",
    "print(\"Lancaster Stemmer\")\n",
    "for word in words:\n",
    "    print(word,' — — ->',lancaster.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8130d13-489c-4d88-b8e5-17a814ce3406",
   "metadata": {},
   "source": [
    "<h1> Practical No. 2(B): Write a program to Implement lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2525a7c3-0294-4c27-861f-4e25d1647ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\BILAL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\BILAL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\BILAL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\BILAL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter words for Lemmatizing My College Name is RDNC and I am doing MSC Computer Science from this college also, I learn Stemming in NLP.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: My -> Lemmatized: My\n",
      "Original: College -> Lemmatized: College\n",
      "Original: Name -> Lemmatized: Name\n",
      "Original: is -> Lemmatized: be\n",
      "Original: RDNC -> Lemmatized: RDNC\n",
      "Original: and -> Lemmatized: and\n",
      "Original: I -> Lemmatized: I\n",
      "Original: am -> Lemmatized: be\n",
      "Original: doing -> Lemmatized: do\n",
      "Original: MSC -> Lemmatized: MSC\n",
      "Original: Computer -> Lemmatized: Computer\n",
      "Original: Science -> Lemmatized: Science\n",
      "Original: from -> Lemmatized: from\n",
      "Original: this -> Lemmatized: this\n",
      "Original: college -> Lemmatized: college\n",
      "Original: also -> Lemmatized: also\n",
      "Original: , -> Lemmatized: ,\n",
      "Original: I -> Lemmatized: I\n",
      "Original: learn -> Lemmatized: learn\n",
      "Original: Stemming -> Lemmatized: Stemming\n",
      "Original: in -> Lemmatized: in\n",
      "Original: NLP -> Lemmatized: NLP\n",
      "Original: . -> Lemmatized: .\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Download necessary NLTK data (WordNet and POS tagger)\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger') # This should download the necessary data\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to map POS tag to WordNet POS format\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "# Example text to lemmatize\n",
    "text = input(\"Enter words for Lemmatizing\")\n",
    "\n",
    "# Tokenize and POS tagging\n",
    "tokens = nltk.word_tokenize(text)\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "# Lemmatize based on POS\n",
    "lemmatized_words = []\n",
    "for word, tag in pos_tags:\n",
    "    wordnet_pos = get_wordnet_pos(tag)\n",
    "    lemmatized_word = lemmatizer.lemmatize(word, wordnet_pos)\n",
    "    lemmatized_words.append((word, lemmatized_word))\n",
    "# Print the result\n",
    "for original, lemmatized in lemmatized_words:\n",
    "    print(f\"Original: {original} -> Lemmatized: {lemmatized}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765fce81-614d-43a0-b534-4d5e61fa7cc1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e28af89a-ce30-4ddd-98dc-4a8d8012f52c",
   "metadata": {},
   "source": [
    "<h1> Practical No. 3: Write a program to Implement a tri-gram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b042ab0-bbeb-4d27-9e18-10d558555b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biagram\n",
      "('My', 'College')\n",
      "('College', 'Name')\n",
      "('Name', 'is')\n",
      "('is', 'RDNC')\n",
      "('RDNC', 'and')\n",
      "('and', 'I')\n",
      "('I', 'am')\n",
      "('am', 'doing')\n",
      "('doing', 'MSC')\n",
      "('MSC', 'Computer')\n",
      "('Computer', 'Science')\n",
      "('Science', 'from')\n",
      "('from', 'this')\n",
      "('this', 'college')\n",
      "('college', 'also,')\n",
      "('also,', 'I')\n",
      "('I', 'learn')\n",
      "('learn', 'Stemming')\n",
      "('Stemming', 'in')\n",
      "('in', 'NLP.')\n",
      "triagram\n",
      "('My', 'College', 'Name')\n",
      "('College', 'Name', 'is')\n",
      "('Name', 'is', 'RDNC')\n",
      "('is', 'RDNC', 'and')\n",
      "('RDNC', 'and', 'I')\n",
      "('and', 'I', 'am')\n",
      "('I', 'am', 'doing')\n",
      "('am', 'doing', 'MSC')\n",
      "('doing', 'MSC', 'Computer')\n",
      "('MSC', 'Computer', 'Science')\n",
      "('Computer', 'Science', 'from')\n",
      "('Science', 'from', 'this')\n",
      "('from', 'this', 'college')\n",
      "('this', 'college', 'also,')\n",
      "('college', 'also,', 'I')\n",
      "('also,', 'I', 'learn')\n",
      "('I', 'learn', 'Stemming')\n",
      "('learn', 'Stemming', 'in')\n",
      "('Stemming', 'in', 'NLP.')\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "sentence = \"My College Name is RDNC and I am doing MSC Computer Science from this college also, I learn Stemming in NLP.\"\n",
    "\n",
    "# Tokenize the sentence into words\n",
    "words = sentence.split()\n",
    "\n",
    "# Create bigrams from the list of words\n",
    "\n",
    "bigrams = ngrams(words, 2) \n",
    "\n",
    "trigrams=ngrams(words, 3)\n",
    "# Print the bigrams\n",
    "print(\"Biagram\")\n",
    "for b in bigrams:\n",
    "    print(b)\n",
    "print(\"triagram\")\n",
    "# Print the trigrams\n",
    "for t  in trigrams  :\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded267f5-0740-46a6-9a56-5afb52decddd",
   "metadata": {},
   "source": [
    "<h1> Practical No. 4(A):  Write a program to Implement PoS tagging using HMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ee5345e-5d1f-4f7b-9bb6-bef8415b6164",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\BILAL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\BILAL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 36.84%\n",
      "\n",
      "POS Tags:\n",
      "My: PRP$\n",
      "College: NNP\n",
      "Name: NNP\n",
      "is: NNP\n",
      "RDNC: NNP\n",
      "and: NNP\n",
      "I: NNP\n",
      "am: NNP\n",
      "doing: NNP\n",
      "MSC: NNP\n",
      "Computer: NNP\n",
      "Science: NNP\n",
      "from: NNP\n",
      "this: NNP\n",
      "college: NNP\n",
      "also: NNP\n",
      ",: NNP\n",
      "I: NNP\n",
      "learn: NNP\n",
      "Stemming: NNP\n",
      "in: NNP\n",
      "NLP: NNP\n",
      ".: NNP\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tag import hmm\n",
    "from nltk.corpus import treebank\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('treebank')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load the Treebank corpus for training and testing\n",
    "train_data = treebank.tagged_sents()[:3000]\n",
    "test_data = treebank.tagged_sents()[3000:]\n",
    "\n",
    "# Train the HMM POS tagger\n",
    "tagger = hmm.HiddenMarkovModelTrainer().train(train_data)\n",
    "\n",
    "# Evaluate the tagger on the test data\n",
    "# Note: The evaluate method is deprecated, use accuracy instead\n",
    "print(f\"Accuracy: {tagger.accuracy(test_data) * 100:.2f}%\")\n",
    "\n",
    "# Example sentence for POS tagging\n",
    "sentence = \"My College Name is RDNC and I am doing MSC Computer Science from this college also, I learn Stemming in NLP.\"\n",
    "\n",
    "# Tokenize and POS tag the sentence\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tags = tagger.tag(tokens)\n",
    "\n",
    "# Print the tagged words\n",
    "print(\"\\nPOS Tags:\")\n",
    "for word, tag in tags:\n",
    "    print(f\"{word}: {tag}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bba03c-5dd7-4755-8c44-61f566bb45b6",
   "metadata": {},
   "source": [
    "<h1> Practical No. 4(B):  Write a program to Implement PoS tagging using Neural Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "993e0657-3b64-4345-aa51-8c9ac10ce44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "POS Tags:\n",
      "My: PRON\n",
      "College: PROPN\n",
      "Name: PROPN\n",
      "is: AUX\n",
      "RDNC: PROPN\n",
      "and: CCONJ\n",
      "I: PRON\n",
      "am: AUX\n",
      "doing: VERB\n",
      "MSC: PROPN\n",
      "Computer: PROPN\n",
      "Science: PROPN\n",
      "from: ADP\n",
      "this: DET\n",
      "college: NOUN\n",
      "also: ADV\n",
      ",: PUNCT\n",
      "I: PRON\n",
      "learn: VERB\n",
      "Stemming: VERB\n",
      "in: ADP\n",
      "NLP: PROPN\n",
      ".: PUNCT\n"
     ]
    }
   ],
   "source": [
    "# If you're running this in a Jupyter Notebook, use this to install spaCy and the model:\n",
    "# %pip install -U spacy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Load the pre-trained spaCy model for English\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Example sentence\n",
    "sentence = \"My College Name is RDNC and I am doing MSC Computer Science from this college also, I learn Stemming in NLP.\"\n",
    "\n",
    "# Process the sentence with spaCy\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# Print the POS tags for each token (word)\n",
    "print(\"\\nPOS Tags:\")\n",
    "for token in doc:\n",
    "    print(f\"{token.text}: {token.pos_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51931a6-5b81-4499-a17e-2572a58a2025",
   "metadata": {},
   "source": [
    "<h1> Practical No. 5:  Write a program to Implement syntactic parsing of a given text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d331cff-ff96-454e-85f2-d662a3929911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing Tree(s):\n",
      "\n",
      "(S\n",
      "  (NP (Det the) (Adj big) (N cat))\n",
      "  (VP (V chased) (NP (Det a) (Adj small) (N mouse))))\n",
      "              S                       \n",
      "      ________|_________               \n",
      "     |                  VP            \n",
      "     |         _________|____          \n",
      "     NP       |              NP       \n",
      "  ___|___     |      ________|_____    \n",
      "Det Adj  N    V    Det      Adj    N  \n",
      " |   |   |    |     |        |     |   \n",
      "the big cat chased  a      small mouse\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import CFG\n",
    "\n",
    "# Ensure required resources are available\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "def main():\n",
    "    # Define the context-free grammar\n",
    "    grammar = CFG.fromstring(\"\"\"\n",
    "        S -> NP VP\n",
    "        NP -> Det Adj N | Det N\n",
    "        VP -> V NP\n",
    "        Det -> 'a' | 'the'\n",
    "        Adj -> 'big' | 'small' | 'furry'\n",
    "        N -> 'cat' | 'mouse' | 'dog'\n",
    "        V -> 'chased' | 'saw' | 'liked'\n",
    "    \"\"\")\n",
    "\n",
    "    # Create a parser\n",
    "    parser = nltk.ChartParser(grammar)\n",
    "\n",
    "    # Define the sentence to parse\n",
    "    sentence = \"the big cat chased a small mouse\".split()\n",
    "    print(\"Parsing Tree(s):\\n\")\n",
    "\n",
    "    # Parse and display the trees\n",
    "    for tree in parser.parse(sentence):\n",
    "        print(tree)\n",
    "        tree.pretty_print()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d13093-b243-4d9c-8521-1f76cbdcede2",
   "metadata": {},
   "source": [
    "<h1> Practical No. 6:  Write a program to Implement dependency parsing of a given text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b41ace5-1544-430b-b204-225cae161f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BILAL\\AppData\\Local\\Temp\\ipykernel_25524\\1938683610.py:3: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython.display\n",
      "  from IPython.core.display import SVG, display  # Enable SVG rendering inline\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word            POS        Dependency Relation\n",
      "========================================\n",
      "A               DET        det                 \n",
      "cat             NOUN       nsubj               \n",
      "sat             VERB       ROOT                \n",
      "on              ADP        prep                \n",
      "a               DET        det                 \n",
      "mat             NOUN       pobj                \n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"16856f74e67446c0a71474ce2a323a62-0\" class=\"displacy\" width=\"1100\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">A</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">cat</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">sat</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">on</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">mat</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-16856f74e67446c0a71474ce2a323a62-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-16856f74e67446c0a71474ce2a323a62-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-16856f74e67446c0a71474ce2a323a62-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,89.5 395.0,89.5 395.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-16856f74e67446c0a71474ce2a323a62-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,179.0 L237,167.0 253,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-16856f74e67446c0a71474ce2a323a62-0-2\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-16856f74e67446c0a71474ce2a323a62-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M570.0,179.0 L578.0,167.0 562.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-16856f74e67446c0a71474ce2a323a62-0-3\" stroke-width=\"2px\" d=\"M770,177.0 C770,89.5 920.0,89.5 920.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-16856f74e67446c0a71474ce2a323a62-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M770,179.0 L762,167.0 778,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-16856f74e67446c0a71474ce2a323a62-0-4\" stroke-width=\"2px\" d=\"M595,177.0 C595,2.0 925.0,2.0 925.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-16856f74e67446c0a71474ce2a323a62-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M925.0,179.0 L933.0,167.0 917.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from IPython.core.display import SVG, display  # Enable SVG rendering inline\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def dependency_parsing(text):\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Print tokens, POS, dependency info\n",
    "    print(f\"{'Word':<15} {'POS':<10} {'Dependency Relation'}\")\n",
    "    print(\"=\"*40)\n",
    "    for token in doc:\n",
    "        print(f\"{token.text:<15} {token.pos_:<10} {token.dep_:<20}\")\n",
    "\n",
    "    # Display SVG directly in output\n",
    "    svg = displacy.render(doc, style=\"dep\", jupyter=False)\n",
    "    display(SVG(svg))  # This shows the SVG inline\n",
    "\n",
    "# Example sentence\n",
    "text = \"A cat sat on a mat\"\n",
    "dependency_parsing(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d96c60b-6014-4650-986a-2ab6f155eb21",
   "metadata": {},
   "source": [
    "<h1> Practical No. 7:  Write a program to Implement Named Entity Recognition (NER)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4eaf4547-1b98-4204-a665-ab083f13da91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entities, Their Labels, and Descriptions:\n",
      "\n",
      "Entity                         Label           Explanation\n",
      "======================================================================\n",
      "Apple Inc.                     ORG             Companies, agencies, institutions, etc.\n",
      "Cupertino                      GPE             Countries, cities, states\n",
      "California                     GPE             Countries, cities, states\n",
      "April 1976                     DATE            Absolute or relative dates or periods\n",
      "Steve Jobs                     PERSON          People, including fictional\n",
      "Steve Wozniak                  PERSON          People, including fictional\n",
      "Ronald Wayne                   PERSON          People, including fictional\n",
      "iPhone                         ORG             Companies, agencies, institutions, etc.\n",
      "2007                           DATE            Absolute or relative dates or periods\n",
      "iPad                           ORG             Companies, agencies, institutions, etc.\n",
      "2010                           DATE            Absolute or relative dates or periods\n",
      "the Apple Watch                ORG             Companies, agencies, institutions, etc.\n",
      "2015                           DATE            Absolute or relative dates or periods\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import textwrap\n",
    "\n",
    "def extract_named_entities(text):\n",
    "    # Load the spaCy English model\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "\n",
    "    print(\"Named Entities, Their Labels, and Descriptions:\\n\")\n",
    "    print(f\"{'Entity':<30} {'Label':<15} {'Explanation'}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        explanation = spacy.explain(ent.label_) or \"No explanation available\"\n",
    "        print(f\"{ent.text:<30} {ent.label_:<15} {explanation}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sample_text = textwrap.dedent(\"\"\"\n",
    "        Apple Inc., a technology company headquartered in Cupertino, California, was founded in April 1976 by Steve Jobs, Steve Wozniak, \n",
    "        and Ronald Wayne, and has since become a global leader in consumer electronics, launching iconic products such as the iPhone in \n",
    "        2007, the iPad in 2010, and the Apple Watch in 2015.\n",
    "    \"\"\")\n",
    "    extract_named_entities(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a879de-7a37-499e-bde7-e02e867be8b4",
   "metadata": {},
   "source": [
    "<h1> Practical No. 8:  Write a program to Implement Text Summarization for the given sample text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bffa44f-0ebb-44e6-85ad-9d9ae372bab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Summary =====\n",
      "\n",
      "Artificial Intelligence (AI) is a branch of computer science that aims to create machines \n",
      "that can perform tasks that would typically require human intelligence. AI has a \n",
      "wide range of applications, from self-driving cars to virtual assistants like Siri and Alexa.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\BILAL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\BILAL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import nltk\n",
    "import re\n",
    "import heapq\n",
    "\n",
    "# Download required nltk data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# 1. Sample text\n",
    "text = \"\"\"\n",
    "Artificial Intelligence (AI) is a branch of computer science that aims to create machines \n",
    "that can perform tasks that would typically require human intelligence. These tasks include \n",
    "speech recognition, decision-making, visual perception, and language translation. AI has a \n",
    "wide range of applications, from self-driving cars to virtual assistants like Siri and Alexa. \n",
    "The growth of AI has sparked debates on ethics and job displacement. Despite challenges, AI \n",
    "continues to be a transformative force in the tech industry.\n",
    "\"\"\"\n",
    "\n",
    "# 2. Text cleaning\n",
    "clean_text = re.sub(r'\\s+', ' ', text)\n",
    "clean_text = re.sub(r'[^a-zA-Z]', ' ', clean_text)\n",
    "clean_text = re.sub(r'\\s+', ' ', clean_text)\n",
    "\n",
    "# 3. Sentence tokenization\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# 4. Word frequency table (excluding stopwords)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "word_frequencies = {}\n",
    "for word in word_tokenize(clean_text.lower()):\n",
    "    if word not in stop_words:\n",
    "        if word not in word_frequencies:\n",
    "            word_frequencies[word] = 1\n",
    "        else:\n",
    "            word_frequencies[word] += 1\n",
    "\n",
    "# 5. Normalize word frequencies\n",
    "max_freq = max(word_frequencies.values())\n",
    "for word in word_frequencies:\n",
    "    word_frequencies[word] = word_frequencies[word] / max_freq\n",
    "\n",
    "# 6. Sentence scoring\n",
    "sentence_scores = {}\n",
    "for sent in sentences:\n",
    "    for word in word_tokenize(sent.lower()):\n",
    "        if word in word_frequencies:\n",
    "            if len(sent.split(' ')) < 30:  # avoid too long sentences\n",
    "                if sent not in sentence_scores:\n",
    "                    sentence_scores[sent] = word_frequencies[word]\n",
    "                else:\n",
    "                    sentence_scores[sent] += word_frequencies[word]\n",
    "\n",
    "# 7. Get top N sentences (summary)\n",
    "summary_sentences = heapq.nlargest(2, sentence_scores, key=sentence_scores.get)\n",
    "summary = ' '.join(summary_sentences)\n",
    "\n",
    "# 8. Print the summary\n",
    "print(\"===== Summary =====\")\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
